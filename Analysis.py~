from numpy import *                                       # for representing arrays of numbers and characters efficiently
from sklearn.ensemble import GradientBoostingClassifier   # existing implementation of gradient-boosted classifier - convenient!
from sklearn.ensemble import RandomForestClassifier       # random forest classifier also implemented
import os   # for passing arguments to the system - necessary for calling R helper functions  
import sys  # necessary for parsing command line input arguments
import csv  # handles csv representation of output - better the microsoft excel for keeping it simple

# evaluation data is data for which there is no known label - this is the real application of the classifier. "Evaluation.dat" is generated by the R helper function, so it is known to exist 
def loadEvaluate():
    with open('Evaluation.dat','r') as f: # open the file containing the data for which we have no labels
        evaluation = [line.strip().replace('"','').lower() for line in f] # parse each line, removing quotation marks and converting everything to lower case
        f.close()     # close the file
    return evaluation # return a list of the surnames

# similar to the function loadEvaluate, but extracts data for which we have known labels and confidences. the files are constructed by the R helper function, so it is known to exist
def loadTrainTest():
    # for all files containing names, confidencees, and labels, strip quotation marks, and convert to lower case while parsing
    with open('Names.dat','r') as f:
        names = [line.strip().replace('"','').lower() for line in f]
        f.close()
    with open('Confidence.dat','r') as f:
        confidence = [line.strip().replace('"','') for line in f]
        f.close()
    with open('Ethnicity.dat','r') as f:
        ethnicity = [line.strip().replace('"','').lower() for line in f]
        f.close()
    return names, confidence, ethnicity # return a list containing, in order, a surname, its confidence, and its known label

# it is necessary to convert the surnames into a vector representation that can be handled by a classifier. function takes a list of surnames and returns binary representation
def createVectors(names):
    n = 3           # number of character n-grams to consider in each name
    ngrams = set()  # initialize set of n-grams
    for name in range(len(names)):                 # for every name in the list
        for j in range(len(names[name]) - n + 1):  # proceed through the name in proper step sizes so as to consider all n-gram sequences
            gram = names[name][j:j+n]              # construct the n-gram
            if gram not in ngrams:                 # if the n-gram has not been seen before, add it to the list of n-grams
                ngrams.add(gram)
    rep = zeros((len(names),len(ngrams))) # initialize vector representation of names of dimensionality (# of names) x (# of n-grams)
    for name in range(len(names)):        # for each name in the list
        i = 0                             # initialize counter (since set ngrams seems to have no useful indexing property) 
        for gram in ngrams:               # for every ngram in the list
            if gram  in names[name]:      # if that ngram appears in the name, then "activate" that element of the name's vector representation
                rep[name,i] = 1
            i += 1      # increase counter by one
    return rep, ngrams  # return both the vector representation and the list of ngrams (will be used for vectorizing the data with unknown labels after classifier is trained)

# create vector representations using the n-grams found earlier for the unlabeled data
def evaluationVectors(evaluation,ngrams): 
    repEval = zeros((len(evaluation),len(ngrams))) # initialize matrix of representation of dimensionality (# of unlabeled names) x (# of ngrams)
    for name in range(len(evaluation)):  # for each name in the list of names
        i = 0                            # initialize counter as before
        for gram in ngrams:              # for every ngram we saw in the training state
            if gram in evaluation[name]: # if it appears in the name, activate that component of the vector representation
                repEval[name,i] = 1
            i += 1
    return repEval # return vector representation of the name

# create a dictionary that maps ethnicities to an integer number - necessary for enumerating classes, but will be converted back at final output stage
def createDictionary(ethnicity):
    c = unique(ethnicity)   # the number of classes that were actually observed
    d = {}                  # set up dictionary - similar to a hashmap (takes keys to values)
    for i in range(len(c)): # for each observed ethnicity label
        d[c[i]] = i         # generate a mapping: label -> number
    return d                # return mapping

# using the mapping created by the function createDictionary, convert the string labels of each name into an integer representation
def createLabels(ethnicity,d):
    label = zeros((len(ethnicity),1)) # every individual obtains a new integer label, so initialize vector of that dimensionality
    for i in range(len(ethnicity)):   # for every individual
        label[i] = d[ethnicity[i]]    # index into the dictionary their ethnicity and obtain its corresponding integer label and assign it to that individual
    return label # return the label

# train and test a gradient-boosted decision tree classifier. takes as input a training set and a testing set and returns the accuracy, the prediction, and the classifier object itself
def createGB(repTr,repTe,labelTr,labelTe):
    c = GradientBoostingClassifier(n_estimators = 20,learning_rate = 1.0,max_depth = 5,random_state = 0).fit(repTr,labelTr) # train the classifier with 100 trees 
    p = atleast_2d(c.predict(repTe)).T         # generate predictions of the classifier for the test data set and convert into numpy 2-D array
    a = sum(p == labelTe) * 1. / len(labelTe)  # calculate the accuracy of the classifier
    return a, p, c # return the accuracy, the predictions, and the classifier

# train and test a random forest classifier. this function is virtually identical to the function createGB, except that the classifier it constructs is different
def createRF(repTr,repTe,labelTr,labelTe):
    c = RandomForestClassifier(n_estimators = 100,n_jobs = -1,max_depth = None,min_samples_split = 5,random_state = 0).fit(repTr,labelTr)
    p = atleast_2d(c.predict(repTe)).T
    a = sum(p == labelTe) * 1. / len(labelTe)
    return a, p, c
    
# in the interest of doing good machine learning research, we construct a training and a testing data set. 
def partitionTrainTest(rep,label):
    index = random.permutation(rep.shape[0]) # randomly reindex the data (this is good practice so as to avoid potential correlations that make classification unfair)
    rep, label = rep[index,:], label[index]  # perform reindexing
    # l = round(.75 * rep.shape[0])          # proportion of data is used for training the classifier - presently set to 75% of the data
    l = 21804                                # value for which testing data begins  
    repTr, repTe = rep[0:l,:], rep[(l+1):,:]        # construct training and testing vector representations 
    labelTr, labelTe = label[0:l], label[(l+1):,:]  # obtain the corresponding labels for the training and testing labels
    return repTr, repTe, labelTr, labelTe # return a data set that has been partitioned into training and testing components

# for readability, in the output we reverse the encoding of the ethnicities as integer values and obtain their string values
def retrieveClass(prediction,dinv):
    l = [] # initialize list of labels
    for i in range(len(prediction)):                    # for every individual whose ethnicity we predicted
        l.append(dinv[int(prediction[i].astype(int))])  # reindex into the dictionary and extract the string using the integer prediction and add it to the list
    return l # return the list

# to obtain results from the classifiers, we output the results to a file, as specified by the input variable "f". pass into the function the individual's names and the classifications
def writeResults(names,gbl,rfl,f):
    d = [names,gbl,rfl]  # set up list to write to file - first column is surname, the gradient boosted and random forest classificcations, respectively
    length = len(d[1])   # length along the top of array - will have to loop over this in order to write file
    header = ['Surname','Gradient Boosted','Random Forest'] # provide headers for the top of the generated csv
    with open(f,'wb') as f:      # create file with name as specified as input
        write = csv.writer(f)    # generate object to write to the file
        write.writerow(header)   # first write the header
        for i in range(length):                # for each column thereafter write the corresponding element of the list - confusing i know (but don't worry about it)
            write.writerow([x[i] for x in d])

# primary function that handles everything
def main():
    print 'welcome to analysis function...'
    command = 'Rscript Process.r --inputfiles ' + sys.argv[1] + ' ' + sys.argv[2] # take as input from the user both a data file of individuals with known labels and unknown labels
    os.system(command)                              # pass this as a system call, which invokes an R script intended to easily generate files without missing data, etc.
    names, confidence, ethnicity = loadTrainTest()  # load data with known labels
    print 'loaded training and testing data...'
    evaluation = loadEvaluate()                     # load data with unknown labels
    print 'loaded evaluation data...'
    d = createDictionary(ethnicity)    # create dictionary to reindex string ethnicities to integers
    dinv = {v:k for k, v in d.items()} # reverse this dictionary mapping for output purposes
    print 'created dictionary and inverse dictionary...'
    rep, ngrams = createVectors(names) # generate vector representations for all ngrams observed in the data
    # rep = rep[:,0:100]
    print 'created vector representations of names...'
    print 'the dimensionality of representation... ',rep.shape 
    label = createLabels(ethnicity,d)  # extract numeric ethnicity labels from corresponding strings for each individual
    repTr, repTe, labelTr, labelTe = partitionTrainTest(rep,label)  # partition the labeled data into training and testing components
    print 'successfully partitioned the data into training and testing components...'
    repEval = evaluationVectors(evaluation,ngrams) # construct vector representation for the unlabeled data - can neither be trained upon nor tested upon - potential for self-taught learning?
    print 'proceeding to learn classifiers...'
    gbflag, rfflag = False, True
    if gbflag:
        print '\tlearning gradient-boosted decision forest...'
        gba, gbp, gbc = createGB(repTr,repTe,labelTr,labelTe)   # construct gradient-boosted classifier
        print '\tgradient-boosted classification tree accuracy:', # report its accuracy to the user
        print gba
        if True:
            gbl = retrieveClass(gbp,dinv) # convert the predictions of the classifer, which are integers, back into the strings that specify ethnicity
    if rfflag:
        print '\tlearning random forest...'
        rfa, rfp, rfc = createRF(repTr,repTe,labelTr,labelTe)  # construct a random forest classifier
        print '\trandom forest classification accuracy:',        # report its accuracy to the user 
        print rfa
        if True:
            rfl = retrieveClass(rfp,dinv) # convert the predictions of the classifer, which are integers, back into the strings that specify ethnicity
    if not gbflag: 
        gbl = array([])
    if not rfflag:
        rfl = array([])
    writeResults(names,gbl,rfl,'TestingResults.csv') # write the results of the classification process to a csv file "TestingResults.csv"
    if False:
        gbEval = atleast_2d(gbc.predict(repEval)).T  # for the unlabeled data, obtain a predicted class for both the gradient boosted classifier and the random forest
        rfEval = atleast_2d(rfc.predict(repEval)).T
        gbLabels = retrieveClass(gbEval,dinv)        # convert the gradient boosted classifier's output into string form
        rfLabels = retrieveClass(rfEval,dinv)        # convert the random forest classifier's output into string form
        writeResults(evaluation,gbLabels,rfLabels,'EvaluationResults.csv') # write the results of the unlabeled classification process to a csv file "EvaluationResults.csv"

if __name__ == '__main__':  # makes function callable from the command line - also convenient
    main()
